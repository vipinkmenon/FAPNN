\section{Background and Related Works}
\label{sec_related}

\begin{figure}[t]
\centering
   \includegraphics[height=0.6\columnwidth]{Figures/pnn.pdf}
   \caption{General layered architecture of PNN}
   \label{fig:pnn}
\end{figure}

Probabilistic Neural Networks (PNNs) are feed forward artificial neural networks developed for solving classification problems \cite{specht1990}.
During network training, PNN stores the statistical parameters of the training data and later uses this information to classify the test inputs.
PNNs have been successfully used in applications such as human emotion \cite{zhang2016pnn} and speech recognition \cite{wrobel2017}, biometric identification \cite{junjea2015}, intrusion detection in IT infrastructure \cite{raman2017,zhao2017}, medical diagnos etc.~\cite{mangayarkarasi2017,thara2016}.


The general architecture of a PNN is as shown in Fig.~\ref{fig:pnn}.
It has a layered structure consisting of input layer, pattern layer, summation layer, and output layer. 
The input layer is responsible for receiving the data to be classified. 
Each data item is represented as a feature vector (p features in the example).
The pattern layer consists of multiple groups of neurons, each group representing one class for classification (two classes in the example).
During training, the weight vector of each pattern neuron $W$ is set to be equal to the features of the corresponding training sample $X$.
Thus each neuron in the pattern layer represents a training sample.
Each input layer neuron is connected with every pattern layer neuron with different weights assigned to the connection. 
The pattern neurons perform dot product between input vector $X$ and weight vector $W$ and applies the result to an activation function. 
Traditional PNNs employ exponential based activation for classification, where the output of $i$th pattern neuron $g_i$ is expressed as Eq. 1 \cite{specht1988}.

\begin{equation}
g_{i}=\frac{1}{\sqrt{2\pi\sigma^{2}}}exp\left(\frac{\boldsymbol{X\cdot W_{i}}-1}{\sigma^{2}}\right)
\end{equation}

Here $\sigma$ is a smoothing parameter, which defines the probability density function (PDF) of the data class. 
Selection of $\sigma$ parameter affects the accuracy of the classification and is generally selected based on heuristic algorithms.
The summation layer, adds and averages the outputs of pattern neurons belonging to each class to produce the probability distribution of this class. 
Finally, the output layer chooses the class with the highest probability as the class of the input sample.


PNNs have the advantages of much faster training process and more accurate results using the minimum Bayesian risk criterion compared with other networks, such as multilayer perceptron networks.
They are faster since the their weights are not \emph{trained} but rather assigned.
In software implementations, PNN requires larger memory for storing the model since the features corresponding to every training data has to be stored and one neuron per train data is required to process the input.
Moreover, as the number of training set increases, the system will need to perform more computation resulting in decrease in speed of classification. 

There have been a few previous implementations of PNNs on FPGA platforms.
Most of these implementations are targeting specific applications and cannot be ported to other applications.
In~\cite{Bu2004} researchers describe an FPGA implementation for bioelectric interface. 
They implement a special PNN called log-linearised Gaussian mixture network, which estimates the posterior probability based on a Gaussian mixture model and a log-linear model.
The exponential activation function is implemented using look-up-tables.
A similar implementation of activation function is adopted in~\cite{Figueiredo1998} also, where FPGA based PNNs are used for multi-spectral image classification.
In ~\cite{Zhou2010} FPGA based PNNs are used for motor cortical decoding in rats.
Here the exponential activation function is implemented as a tailor series expansion using look-up-tables.
A large number of resources are utilised for this implementation.
This work was later improved by~\cite{Zhu2010}, where the activation function is implemented using CORDIC IP cores.

Since the classification accuracy of PNNs is directly proportional to the number of neurons in pattern layer, it is important to limit the resource requirement of individual neurons and to maximise the total number of neurons.
In the following section we discuss different approximations to control the resource utilisation and its impact on the overall system performance.
Although these techniques are described taking PNNs as an example, same or similar techniques can be applied to other neural networks also.
%\cite{,,Ovtcharov2015,,Shima2007,,Zhang2015}