\section{Introduction}
\label{sec_intro}

Recent years have witnessed rapid adoption of neural networks in varied domains such as speech recognition~\cite{Deng2013}, electrical energy management~\cite{Houria2015}, stock market forecasting~\cite{Ticknor2015}, medicine~\cite{Azar2013} etc. 
Although inherently parallel in nature, almost all these implementations are software-based running on a standard computer.
As the complexity of the networks and applications involve, software only implementations find it difficult to meet the performance requirements due to the limited scaling of general purpose processor performance.
Pure and custom hardware implementations can provide considerable speed-up in this regard due to their parallel execution model. 
Still the adoption in this regard is limited owing to a number of challenges faced during hardware implementation of neural networks.

The high cost associated with ASIC mask design and the limited commercial demand limits the full custom implementation of these networks.
One of the promising arenas is the implementation of the network in a cloud infrastructure~\cite{Yuan2014} and providing it as a service to the users.
Since the user specifications of the network can greatly vary, a complete custom hardware solution is not feasible in this scenario either.
Field programmable gate arrays (FPGAs) are promising in this scenario due to their flexible implementation capabilities.
They are capable of providing much higher throughput compared to software counterparts along with reconfigurability.

Another limitation of hardware implementation is the limited design automation support availability.
For developing an FPGA based network, the designer should be an expert on FPGA architectural details as well as a hardware description language (HDL).
Recently introduced high-level synthesis languages enable designing at a higher abstraction (such as in C programming language), but their performance is much lower compared to HDL-based designs.
They are also not capable of extracting full capabilities of low-level FPGA primitives.
The designer also needs to build the communication infrastructure to send and receive data between the FPGA and a host computer, and the speed of this link is crucial in achieving high performance.
Contrary to this, a number of design tools are available for software-based implementation of neural networks.
This enables rapid prototyping of applications exploiting this architecture and easy adoption by the mass.

Another stumbling block for FPGAs is their limited resource availability.
A given FPGA chip has limited circuit building blocks and it is simply incapable of implementing circuits requiring more than these resources.
Many of the neural network implementations require complex mathematical functions (such as exponential and logarithmic functions), which are highly resource intensive.
Also floating point based arithmetic operations encountered in neural networks are difficult to implement in pure hardware.
Appropriate hardware-level approximations might be capable of mitigating these limitations, while keeping the overall error within tolerable limits. 

In this work we are addressing the above discussed issues faced during FPGA-based neural network implementations.
The main contributions of this work are
\begin{itemize}
\item Design and implementation of an open-source FPGA-based neural network prototyping platform
\item Abstraction of low-level architecture dependent implementation details through design automation scripts and open-source IP cores
\item Example implementation of a probability neural network (PNN) library called FAPNN with hardware-centric optimisations for better throughput and lower resource consumption
\end{itemize}

The remainder of this paper is organised as follows.
Section~\ref{sec_related} discusses the background and the previous FPGA-based PNN implementations, Section~\ref{sec_arch} discusses the proposed FPGA-based neural network implementation platform, Section~\ref{sec_results} discusses the implementation results and performance metrics, Section~\ref{sec_conclusion} concludes the paper and provides some of the future research directions.