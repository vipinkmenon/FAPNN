%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf]{acmart}
%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn


\usepackage{booktabs} % For formal tables


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
  Paso, Texas USA} 
\acmYear{1997}
\copyrightyear{2016}


\acmArticle{4}
\acmPrice{15.00}



\begin{document}
\title{Approximate Probabilistic Neural Networks with Gated Threshold Logic}




\begin{abstract}
Probabilistic Neural Network (PNN) is one of the feed-forward artificial neural networks developed for solving classification problems. This paper proposes a hardware implementation of an approximated PNN (APNN) algorithm in which the conventional exponential function of the PNN was replaced with gated threshold logic. The weights of the PNN are approximated using a memristive crossbar architecture using memristors. In particular, the proposed algorithm performs normalization of the training weights, and quantization into 16 levels which significantly reduces the complexity of the circuit. The result of five-fold cross-validation on Iris dataset demonstrated the accuracy of 98\%.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
 
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{PNN, APNN, crossbar.}



\maketitle


\section{Introduction}
Probabilistic Neural Network (PNN) is one of the feed forward artificial neural networks developed for classification problems \cite{specht1990probabilistic}. The main purpose of such network is sorting the input data by the help of previously inserted training samples. This type of network retrieves the statistical parameters of the training sample and using these statistical information it classifies the input. In general, comparing to heuristic approach based neural networks, such as backpropagation neural network, the training of the PNN can be implemented faster \cite{specht1990probabilistic}. The primary reason is that an adaptation of the network parameters of backpropagation neural network is performed incrementally. For the PNN, the training set is directly used for computing the probability density functions (PDF) of each data category, which is further used for data classification. Such PNN classification system has wide application in power system fault detection \cite{mishra2016classification}, human emotion \cite{zhang2016pnn} and speech recognition \cite{wrobel2017using}, biometric identification \cite{junjea2015dynamic}, intrusion detection in IT infrastructure \cite{raman2017hypergraph}, \cite{zhao2017intrusion}, and diagnosing patients \cite{mangayarkarasi2017pnn}, \cite{thara2016brain}.

One of the challenging tasks of PNN implementation is an architecture simplification. Some training samples of the pattern layer are usually redundant which enlarge the network structure and increase the computational complexity. Hence, a modified version of the PNN presented in \cite{burrascano1991learning} applies the learning vector quantization (LVQ) approach by which the number of neurons in the pattern layer can be reduced, i.e. the number of neurons is not equal to the number of training weights of particular class, as an alternative it is equal to the processing elements per each class. However, with the reduction of the number of neurons, the classification accuracy drops as well. Likewise, a small network structure is presented in \cite{mao2000probabilistic} where the forward regression orthogonal transform algorithm is applied to construct the pattern layer. It is necessary to choose representative neurons by defining their respective importance indexes. In addition, the classification error rate is computed for each class which is also considered for selecting training samples. Secondly, the smoothing parameter value is determined using the genetic algorithm. It was concluded that with the proposed algorithm the small network structure can be achieved with the reasonable classification accuracy. Unlike the conventional PNN and LVQ based PNN, an improved architecture for the PNN proposed in \cite{chandra2011improved} consists of three layers merely. This was achieved taking into consideration the fact that the sum of all Gaussian samples performed in the summation layer is non-linear which makes the PNN architecture complex. The problem is solved by using a mean comprising logarithmic function (f-mean) which can eliminate exponential terms of Gaussians. The classification accuracy of this architecture is superior and the computational time is significantly reduced, yet, a hardware design of the proposed model is not developed. Moreover, a new architecture of the PNN based on modifying the learning parameter is proposed in \cite{aibe2002probabilistic}. The sigma parameter of the PNN affects to the classification performance, however, it is often problematic to define this parameter in real time due to the long computation time of the PNN. Nevertheless, the proposed algorithm is able to perform learning procedure faster than conventional methods do. Specifically, it is targeted to solve a problem with implementing the learning algorithm to the VLSI. However, the PNN architectures in most of those papers include exponential function which causes difficulties for hardware implementation with crossbar architecture.

This paper proposes a novel design of PNN, namely approximate PNN (APNN) with gated threshold logic. Comparing to conventional PNN, the exponential function is replaced by the threshold logic, since implementation of the exponential operator in crossbar architecture is complex and inaccurate. Therefore, threshold is used as a decision mechanism for classification. The threshold σ selection is out of the scope of this paper. The following sections will be focused on the PNN background, design of APNN, its hardware implementation and performance comparison with conventional PNN.

\section{PROBABILISTIC NEURAL NETWORKS}
The PNN consists of neurons distributed over four layers, namely input layer, pattern layer, summation layer, and output layer (Figure 1). The input layer of the PNN is responsible for receiving the data to be sorted. Each input layer neuron is connected with every pattern layer neuron with different weights assigned to the connection. In the training phase, the weight vector of each pattern unit W is set to be equal to initial training sample X patterns. Therefore, the number of pattern neurons needs to be equal to the number of training samples. Each pattern neuron performs dot product between input vector X and weight vector W. Consequently, the result of the dot product is applied to activation function. Comparing to other sigmoid function based neural networks, PNN employs exponential based activation for classification. The output of ith pattern neuron is expressed in Eq. 1 \cite{specht1988probabilistic}.

\begin{equation}
g_{i}=\frac{1}{\sqrt{2\pi\sigma^{2}}}exp\left(\frac{\boldsymbol{X\cdot W_{i}}-1}{\sigma^{2}}\right)
\end{equation}


where σ is a smoothing parameter, which defines the PDF of the data class. Selection of σ parameter affects the accuracy of the classification. However, it was proven that even extreme values of σ, such as 0 and infinity, will not significantly reduce the accuracy. The next layer of the PNN is a summation layer, where the outputs of pattern neurons belonging to one class are summed to produce the probability distribution of this class. Finally, the output layer neurons perform another summation with different weights to select to which class the input belongs. Although the PNN has advantage of faster classification than other neural networks, the number of neurons rise as the number of training set increases, thus requiring large space for storing training sets. Moreover, as the number of training set increases, the system will need to perform more computation resulting in decrease in speed of classification. Nevertheless, in some particular moderate scale classification task, the PNN displayed 200000 times faster execution than backpropagation neural networks \cite{specht1990probabilistic}. 

\section{}


\bibliographystyle{ACM-Reference-Format}
\bibliography{APNN.bib} 

\end{document}
